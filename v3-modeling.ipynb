{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "from numpy import log, mean\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'pandas', 'transformers==2.4.1'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-03 09:33:39 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "=========================\n",
      "\n",
      "2020-08-03 09:33:39 INFO: Use device: cpu\n",
      "2020-08-03 09:33:39 INFO: Loading: tokenize\n",
      "2020-08-03 09:33:39 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# trying the google translate api\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "def translate(text):\n",
    "    return translator.translate(text).text\n",
    "\n",
    "# need to load russian spacy model\n",
    "import stanza\n",
    "from spacy_stanza import StanzaLanguage\n",
    "\n",
    "snlp = stanza.Pipeline(lang=\"ru\",processors='tokenize')\n",
    "nlp = StanzaLanguage(snlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['blogs_2013', 'fiction', 'public', 'science', 'speech'])\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pickle\n",
    "rnc = pickle.load(open(\"/Users/paigelee/Desktop/clancy/code/rnc-v4.pickle\", \"rb\" ) )\n",
    "print(rnc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE LENGTHS:\n",
      "median: 66.0\n",
      "average: 179.7679856115108\n"
     ]
    }
   ],
   "source": [
    "# average or median length of excerpt?\n",
    "import statistics\n",
    "\n",
    "file_lengths = []\n",
    "for folder in rnc.keys():\n",
    "    for file in rnc[folder].keys():\n",
    "        file_length = len(rnc[folder][file]['contents']['sentencetext'])\n",
    "        file_lengths.append(file_length)\n",
    "        \n",
    "median = statistics.median(file_lengths)\n",
    "avg = mean(file_lengths)\n",
    "print('FILE LENGTHS:')\n",
    "print('median:', median)\n",
    "print('average:', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform texts? make excerpts\n",
    "def make_excerpts(rnc, excerpt_length, shuffled=True):\n",
    "    excerpts = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in rnc.keys():\n",
    "        for file in rnc[folder].keys():\n",
    "            metadict = rnc[folder][file]['metadata']\n",
    "            excerpt = \"\"\n",
    "            i = 0\n",
    "            if 'sex' in metadict.keys():\n",
    "                contents = rnc[folder][file]['contents']['sentencetext']\n",
    "                gender = metadict['sex']\n",
    "                for sentence in contents:\n",
    "                    if i < excerpt_length:\n",
    "                        excerpt = excerpt + sentence\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        excerpts.append(excerpt)\n",
    "                        labels.append(gender)\n",
    "                        excerpt = \"\"\n",
    "                        i = 0\n",
    "    if shuffled:\n",
    "        random.seed(4)\n",
    "        tuples = list(zip(excerpts, labels))\n",
    "        random.shuffle(tuples)\n",
    "        excerpts, labels = zip(*tuples)\n",
    "\n",
    "    return excerpts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# excerpts: 9755\n",
      "# labels: 9755\n"
     ]
    }
   ],
   "source": [
    "excerpts, labels = make_excerpts(rnc, 5)\n",
    "print('# excerpts:', len(excerpts))\n",
    "print('# labels:', len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# male authored texts: 7132\n",
      "# female authored texts: 2623\n"
     ]
    }
   ],
   "source": [
    "male_labeled = []\n",
    "female_labeled = []\n",
    "\n",
    "# how many male-labeled and female=labeled texts\n",
    "for index in range(len(labels)):\n",
    "    if labels[index] == 'муж':\n",
    "        male_labeled.append(excerpts[index])\n",
    "    else:\n",
    "        female_labeled.append(excerpts[index])\n",
    "print('# male authored texts:', len(male_labeled))\n",
    "print('# female authored texts:', len(female_labeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "жен wives\n",
      "\n",
      "Всю свою жизнь, а именно 22 года, кот прожил в семье Бабаевых( г. Баку).В 1977 г. дочь Бабаевых Гюльчохра, тогда ещё школьница, принесла домой двухмесячного котёнка, который оказался очень общительным и сообразительным.Вскоре он стал любимцем всей семьи, но наибольшее внимание уделяла ему Гюльчохра Бабаева.Она много времени проводила с животным, постоянно с ним разговаривала.В результате многолетнего речевого( азербайджанский язык) контакта с человеком и тренинга животное научилось адекватно реагировать на обращённые к нему слова и правильно отвечать на вопросы.\n",
      "\n",
      "All his life, namely 22 years, the cat lived in the Babayev family (Baku). In 1977, the Babayevs' daughter Gulchohra, then still a schoolgirl, brought home a two-month-old kitten, which turned out to be very sociable and quick-witted. Soon he became a favorite of the whole family. But Gulchohra Babayeva paid the most attention to him. She spent a lot of time with the animal, constantly talked to him. As a result of many years of verbal (Azerbaijani) contact with a person and training, the animal learned to adequately respond to words addressed to him and correctly answer questions.\n"
     ]
    }
   ],
   "source": [
    "# see a random excerpt and its label\n",
    "i = random.choice(range(len(excerpts)))\n",
    "sent = excerpts[i]\n",
    "gender  = labels[i]\n",
    "print(gender, translate(gender))\n",
    "print()\n",
    "print(sent)\n",
    "print()\n",
    "print(translate(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['всю', 'свою', 'жизнь', 'а', 'именно', 'года', 'кот', 'прожил', 'в', 'семье', 'бабаевых', 'баку', 'в', 'дочь', 'бабаевых', 'гюльчохра', 'тогда', 'ещё', 'школьница', 'принесла', 'домой', 'двухмесячного', 'котёнка', 'который', 'оказался', 'очень', 'общительным', 'и', 'сообразительным', 'вскоре', 'он', 'стал', 'любимцем', 'всей', 'семьи', 'но', 'наибольшее', 'внимание', 'уделяла', 'ему', 'гюльчохра', 'много', 'времени', 'проводила', 'с', 'животным', 'постоянно', 'с', 'ним', 'разговаривала', 'в', 'результате', 'многолетнего', 'речевого', 'азербайджанский', 'язык', 'контакта', 'с', 'человеком', 'и', 'тренинга', 'животное', 'научилось', 'адекватно', 'реагировать', 'на', 'обращённые', 'к', 'нему', 'слова', 'и', 'правильно', 'отвечать', 'на', 'вопросы']\n"
     ]
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "def simple_tokenizer(doc):\n",
    "    parsed = nlp(doc)\n",
    "    return([t.lower_ for t in parsed if t.is_alpha])\n",
    "# testing simple tokenizer\n",
    "print(simple_tokenizer(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory analysis on vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make different vectors\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer, max_df=.99, min_df=.005)\n",
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer, max_df=.99, min_df=.005)\n",
    "nmf = NMF(n_components=10)\n",
    "lda = LatentDirichletAllocation(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf for nmf\n",
    "tfidf_vecs = tfidf.fit_transform(excerpts).toarray()\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)\n",
    "# count for lda\n",
    "count_vecs = cv.fit_transform(excerpts).toarray()\n",
    "lda_vecs = lda.fit_transform(count_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9755, 853)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9755, 853)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9755, 87456)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/paigelee/Desktop/ismt-117/final-project/vecs_853\n"
     ]
    }
   ],
   "source": [
    "cd vecs_853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy arrays as csv files\n",
    "from numpy import savetxt\n",
    "# define data\n",
    "# save to csv file\n",
    "for vecs, name in [(tfidf_vecs, 'tfidf_vecs'), (nmf_vecs, 'nmf_vecs'), (count_vecs, 'count_vecs'), (lda_vecs, 'lda_vecs')]:\n",
    "    savetxt(name + '.csv', vecs, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('и', 19036)\n",
      "('в', 16744)\n",
      "('не', 9388)\n",
      "('на', 7664)\n",
      "('что', 6333)\n",
      "('с', 5776)\n",
      "('а', 4841)\n",
      "('я', 4688)\n",
      "('как', 3769)\n",
      "('это', 3301)\n"
     ]
    }
   ],
   "source": [
    "# get most frequent words in count vecs\n",
    "sum_words = count_vecs.sum(axis=0) \n",
    "words_freq = [(word, sum_words[idx]) for word, idx in list(cv.vocabulary_.items())]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "for idx in range(10):\n",
    "    print(words_freq[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female: ('в', 259.46661631964685) \tmale: ('и', 840.6485365813268)\n",
      "female: ('и', 257.87422254389753) \tmale: ('в', 760.8955912877203)\n",
      "female: ('не', 177.5003221922877) \tmale: ('не', 488.8817432348205)\n",
      "female: ('на', 143.57850832478803) \tmale: ('на', 456.6732561431596)\n",
      "female: ('с', 126.93429321471551) \tmale: ('я', 400.8459389269238)\n",
      "female: ('а', 126.50432571409367) \tmale: ('с', 380.48595083832)\n",
      "female: ('что', 125.99934734230546) \tmale: ('что', 379.07922961491363)\n",
      "female: ('я', 115.0657606609694) \tmale: ('а', 333.9767010535833)\n",
      "female: ('это', 109.82014633150047) \tmale: ('он', 273.90197370528057)\n",
      "female: ('у', 95.23245844435249) \tmale: ('как', 267.46363631689337)\n"
     ]
    }
   ],
   "source": [
    "# summing over female authored texts\n",
    "female_sum_words = tfidf_vecs[is_female].sum(axis=0)\n",
    "female_words_freq = [(word, female_sum_words[idx]) for word, idx in list(tfidf.vocabulary_.items())]\n",
    "female_words_freq = sorted(female_words_freq, key = lambda x: x[1], reverse=True)\n",
    "# summing over male authored texts\n",
    "male_sum_words = tfidf_vecs[~is_female].sum(axis=0)\n",
    "male_words_freq = [(word, male_sum_words[idx]) for word, idx in list(tfidf.vocabulary_.items())]\n",
    "male_words_freq = sorted(male_words_freq, key = lambda x: x[1], reverse=True)\n",
    "for idx in range(10):\n",
    "    print('female:', words_freq[idx], '\\tmale:', male_words_freq[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_female: [False False False False False False  True False False  True]\n",
      "is_female_array: [0 0 0 0 0 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# convert labels from string to bool to int\n",
    "is_female_list = [gender == 'жен' for gender in labels]\n",
    "is_female = np.array(is_female_list)\n",
    "int_labels = [int(boolean) for boolean in is_female]\n",
    "is_female_array = np.array(int_labels)\n",
    "print('is_female:', is_female[:10])\n",
    "print('is_female_array:', is_female_array[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_vecs female ['как', 'это', 'я', 'а', 'с', 'что', 'на', 'не', 'в', 'и']\n",
      "count_vecs male ['он', 'как', 'а', 'я', 'с', 'что', 'на', 'не', 'в', 'и']\n",
      "tfidf_vecs female ['у', 'это', 'я', 'что', 'а', 'с', 'на', 'не', 'и', 'в']\n",
      "tfidf_vecs male ['как', 'он', 'а', 'что', 'с', 'я', 'на', 'не', 'в', 'и']\n"
     ]
    }
   ],
   "source": [
    "# if not, try this:\n",
    "# get top x words\n",
    "top_words = 10\n",
    "# for pos/neg set\n",
    "for vectorizer, vecs, to_print  in [(cv, count_vecs, 'count_vecs'), (tfidf, tfidf_vecs, 'tfidf_vecs')]:\n",
    "    for s, gender in [(is_female, 'female'), (~is_female, 'male')]:    \n",
    "        # sum counts\n",
    "        s_sum = vecs[s].sum(axis=0)\n",
    "        # sort arguments\n",
    "        s_sorted = np.argsort(s_sum)\n",
    "        # print top words\n",
    "        print(to_print, gender, [vectorizer.get_feature_names()[x] for x in s_sorted[-top_words:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVC on Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf (9755, 853)\n",
      "cv (9755, 853)\n",
      "nmf (9755, 10)\n",
      "lda (9755, 10)\n"
     ]
    }
   ],
   "source": [
    "print('tfidf', tfidf_vecs.shape)\n",
    "print('cv', count_vecs.shape)\n",
    "print('nmf',nmf_vecs.shape)\n",
    "print('lda', lda_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paigelee/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7642637512811753\n",
      "0.7840792620430475\n",
      "0.743423300307482\n",
      "0.7423983600956611\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "random_state = 42\n",
    "test_size = 0.3\n",
    "for v in [count_vecs, tfidf_vecs, nmf_vecs, lda_vecs]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(v, labels, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=random_state)\n",
    "    svc = LinearSVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    print(accuracy_score(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paigelee/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7639221045439016\n",
      "0.7840792620430475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paigelee/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7560642295866075\n",
      "0.7584557567475231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paigelee/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "for v in [np.concatenate([count_vecs, lda_vecs], axis=1),\n",
    "          np.concatenate([tfidf_vecs, nmf_vecs], axis=1),\n",
    "          np.concatenate([count_vecs, tfidf_vecs, lda_vecs, nmf_vecs], axis=1),\n",
    "          np.concatenate([count_vecs, tfidf_vecs], axis=1)]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(v, labels, \n",
    "                                                        test_size=test_size, random_state=42)\n",
    "    svc = LinearSVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    print(accuracy_score(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempting an LSTM model using RusVectores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_index(docs, vocab):\n",
    "    # transform docs into series of indices\n",
    "    docs_idxs = []\n",
    "    for d in docs:\n",
    "        w_idxs = []\n",
    "        for w in d:\n",
    "            if w in vocab:\n",
    "                w_idxs.append(vocab[w])\n",
    "            else:\n",
    "                # unknown token = 1\n",
    "                w_idxs.append(1)\n",
    "        docs_idxs.append(w_idxs)\n",
    "    return(docs_idxs)\n",
    "\n",
    "def pad_sequence(seqs, seq_len=200):\n",
    "    # function for adding padding to ensure all seq same length\n",
    "    features = np.zeros((len(seqs), seq_len),dtype=int)\n",
    "    for i, seq in enumerate(seqs):\n",
    "        if len(seq) != 0:\n",
    "            features[i, -len(seq):] = np.array(seq)[:seq_len]\n",
    "    return features\n",
    "\n",
    "class SentimentNet(nn.Module):\n",
    "    # sentiment classifier with single LSTM layer + Fully-connected layer, sigmoid activation and dropout\n",
    "    # adapted from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n",
    "    def __init__(self,\n",
    "                 weight_matrix=None,\n",
    "                 vocab_size=1000, \n",
    "                 output_size=1,  \n",
    "                 hidden_dim=512,\n",
    "                 embedding_dim=400, \n",
    "                 n_layers=2, \n",
    "                 dropout_prob=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        # size of the output, in this case it's one input to one output\n",
    "        self.output_size = output_size\n",
    "        # number of layers (default 2) one LSTM layer, one fully-connected layer\n",
    "        self.n_layers = n_layers\n",
    "        # dimensions of our hidden state, what is passed from one time point to the next\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # initialize the representation to pass to the LSTM\n",
    "        self.embedding, embedding_dim = self.init_embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            weight_matrix)\n",
    "        # LSTM layer, where the magic happens\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=dropout_prob, batch_first=True)\n",
    "        # dropout, similar to regularization\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        # sigmoid activiation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # forward pass of the network\n",
    "        batch_size = x.size(0)\n",
    "        # transform input\n",
    "        embeds = self.embedding(x)\n",
    "        # run input embedding + hidden state through model\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # reshape\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        # dropout certain pct of connections\n",
    "        out = self.dropout(lstm_out)\n",
    "        # fully connected layer\n",
    "        out = self.fc(out)\n",
    "        # activation function\n",
    "        out = self.sigmoid(out)\n",
    "        # reshape\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        # return the output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_embedding(self, vocab_size, embedding_dim, weight_matrix):\n",
    "        # initializes the embedding\n",
    "        if weight_matrix is None:\n",
    "            if vocab_size is None:\n",
    "                raise ValueError('If no weight matrix, need a vocab size')\n",
    "            # if embedding is a size, initialize trainable\n",
    "            return(nn.Embedding(vocab_size, embedding_dim),\n",
    "                   embedding_dim)\n",
    "        else:\n",
    "            # otherwise use matrix as pretrained\n",
    "            weights = torch.FloatTensor(weight_matrix)\n",
    "            return(nn.Embedding.from_pretrained(weights),\n",
    "                  weights.shape[1])\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # initializes the hidden state\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
    "                  torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
    "        return hidden\n",
    "    \n",
    "def train_model(model, train_loader, val_loader, model_params, training_params):\n",
    "    # utility for running the training process\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                 lr=training_params['learning_rate'])\n",
    "    epochs = training_params['epochs']\n",
    "    batch_size = training_params['batch_size']\n",
    "    # print options\n",
    "    counter = 0\n",
    "    print_every = 5\n",
    "    clip = 5\n",
    "    valid_loss_min = np.Inf\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        h = model.init_hidden(batch_size)\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "            h = tuple([e.data for e in h])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            output, h = model(inputs, h)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            if counter%print_every == 0:\n",
    "                val_h = model.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                for inp, lab in val_loader:\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    inp, lab = inp.to(device), lab.to(device)\n",
    "                    out, val_h = model(inp, val_h)\n",
    "                    val_loss = criterion(out.squeeze(), lab.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                model.train()\n",
    "                print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "                if np.mean(val_losses) <= valid_loss_min:\n",
    "                    torch.save(model.state_dict(), './state_dict.pt')\n",
    "                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                    valid_loss_min = np.mean(val_losses)\n",
    "    return(model)\n",
    "    \n",
    "def assess_accuracy(model, test_loader, model_params, training_params):\n",
    "    # utility for assessing accuracy\n",
    "    batch_size = training_params['batch_size']\n",
    "    model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "    h = model.init_hidden(batch_size)\n",
    "    num_correct = 0\n",
    "    model.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "        h = tuple([each.data for each in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output, h = model(inputs, h)\n",
    "        # takes output, rounds to 0/1\n",
    "        pred = torch.round(output.squeeze())\n",
    "        # take the correct labels, check against preds\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "        # sum the number of correct\n",
    "        num_correct += np.sum(correct)\n",
    "    # calc accuracy\n",
    "    test_acc = num_correct/len(test_loader.dataset)\n",
    "    print('LSTM accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 6828 , y_train: 6828\n",
      "X_val: 2048 , y_val: 2048\n",
      "X_test: 879 , y_test: 879\n",
      "['Глядели и вздыхали, вспоминая былое.Так было и нынче.Яков через отворённую форточку приказывал:Сапоги чисто промывайте!Не тягайте грязь!', 'Мы спустились, и Юрка из предосторожности поднял лестницу.В силках билась птица с разноцветным хвостом.Мила тут же, в зарослях, ощипала её, и мы отправились обратно.У меня нет слов, чтобы описать тот ужас, который охватил нас, когда мы услышали на поляне голоса пиратов.Скрываясь в высокой траве, нам удалось добраться незамеченными до самой поляны.'] [0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "random_state = 42\n",
    "test_size = 0.3\n",
    "\n",
    "# separate excerpts (lists) of strings and is_female_array of booleans into train, test, split\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(excerpts, is_female_array, test_size=test_size, random_state=random_state)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=test_size)\n",
    "\n",
    "# check sizes\n",
    "print('X_train:', len(X_train), ', y_train:', len(y_train))\n",
    "print('X_val:', len(X_val), ', y_val:', len(y_val))\n",
    "print('X_test:', len(X_test), ', y_test:', len(y_test))\n",
    "print(X_train[:2], y_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-fe05cfed4395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# tokenize samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparsed_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimple_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparsed_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimple_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mparsed_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimple_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-fe05cfed4395>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# tokenize samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparsed_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimple_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparsed_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimple_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mparsed_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimple_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ffca11160498>\u001b[0m in \u001b[0;36msimple_tokenizer\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initialize tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimple_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# testing simple tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             )\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy_stanza/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mthese\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmapped\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \"\"\"\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvecs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_token_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vector\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy_stanza/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHEAD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"uint64\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msnlp_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tokenize samples\n",
    "parsed_train = [simple_tokenizer(str(d)) for d in X_train]\n",
    "parsed_val = [simple_tokenizer(str(d)) for d in X_val]\n",
    "parsed_test = [simple_tokenizer(str(d)) for d in X_test]\n",
    "\n",
    "# assert parsed lists are looking good\n",
    "print('len parsed_train:', len(parsed_train))\n",
    "print(parsed_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parsed_lists.txt', 'w') as t:\n",
    "    t.write(parsed_train)\n",
    "    t.write(parsed_val)\n",
    "    t.write(prased_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this formulation works if you have previously tokenized\n",
    "cv = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False, min_df=0.01)\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False, min_df=0.01)\n",
    "\n",
    "# **important** just fit on trained: prevents information from test in training \n",
    "cv_train = cv.fit_transform(parsed_train)\n",
    "tfidf_train = tfidf.fit_transform(parsed_train)\n",
    "\n",
    "# get out the vocab (same for tfidf)\n",
    "vocab = cv.vocabulary_\n",
    "print(\"Size of vocab:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sketchy area of setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to adapt vocab, leave space for padding\n",
    "vocab = cv.vocabulary_\n",
    "vocab = dict([(v, vocab[v]+2) for v in vocab])\n",
    "\n",
    "# is this a word2vec thing? hopefully not\n",
    "vocab['_UNK'] = 1\n",
    "vocab['_PAD'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does 'vocab look like'? can you get items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the glove vecs accordingly\n",
    "# populating an array with the vector values of stuff\n",
    "rus_vecs = np.zeros(shape=(len(vocab), 300))\n",
    "for k, v in vocab.items():\n",
    "    if k in model.vocab.keys():\n",
    "        rus_vecs[v] = model.wv[word]\n",
    "    else:\n",
    "        rus_vecs[v] = np.zeros(300).reshape(300)\n",
    "# NOT SURE IF THIS IS CORRECT YET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ok less sketchy **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'weight_matrix': rus_vecs,\n",
    "               'output_size': 1,\n",
    "               'hidden_dim': 512,\n",
    "               'n_layers': 2,\n",
    "               'embedding_dim': 400,\n",
    "               'dropout_prob': 0.2}\n",
    "training_params = {'learning_rate': 0.005,\n",
    "                  'epochs': 1,\n",
    "                  'batch_size': 100}\n",
    "\n",
    "# create padded datasets for train, val, test\n",
    "parsed_train = doc_to_index(parsed_train, vocab)\n",
    "padded_train = pad_sequence(parsed_train)\n",
    "\n",
    "parsed_val = doc_to_index(parsed_val, vocab)\n",
    "padded_val = pad_sequence(parsed_val)\n",
    "\n",
    "parsed_test = doc_to_index(parsed_test, vocab)\n",
    "padded_test = pad_sequence(parsed_test)\n",
    "\n",
    "# construct datasets for loading by PyTorch\n",
    "train_data = TensorDataset(torch.from_numpy(padded_train), torch.from_numpy(y_train))\n",
    "val_data = TensorDataset(torch.from_numpy(padded_val), torch.from_numpy(y_val))\n",
    "test_data = TensorDataset(torch.from_numpy(padded_test), torch.from_numpy(y_test))\n",
    "\n",
    "# you'll need to re-create loaders for changes to batch size\n",
    "batch_size = training_params['batch_size']\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,\n",
    "                         drop_last=True) # this is to keep the size consistent\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size,\n",
    "                       drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,\n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(SentimentNet(**model_params), train_loader, val_loader, model_params, training_params)\n",
    "assess_accuracy(SentimentNet, test_loader, model_params, training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
